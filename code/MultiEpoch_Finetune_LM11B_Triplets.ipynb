{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e58a3-c400-4b83-a865-17249edec364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install peft wandb\n",
    "# !{sys.executable} -m pip install pandas>=2.0.0\n",
    "import os\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # for 1B\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5742e-3e82-491a-8fa5-90b613457b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import MllamaForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datetime import datetime\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac3fdfd-43c1-4f48-854d-21bb9fb726ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# base_model = \"meta-llama/Llama-3.2-90B-Vision-Instruct\"\n",
    "# base_model = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "base_model = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "# base_model = \"meta-llama/Llama-3.2-1B\"\n",
    "# base_model = \"meta-llama/Llama-3.2-3B\"\n",
    "# base_model = \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "# base_model = \"meta-llama/Meta-Llama-3.1-405B\"\n",
    "new_model = \"llama-32-11B_chat-building\"\n",
    "import torch\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "# !{sys.executable} -m pip install bitsandbytes\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2809a87-82b7-4ae6-8a8c-55d80f3d5a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0cbb63668a4d01a3226ad455ae48ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "# 11B \n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config, # BitsAndBytesConfig(load_in_8bit=True), #bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7c21e-7a28-4e02-ac20-3693ef71bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#  with open('/srv/data1/fuxiaoqin/Downloads/EngineeringReference1_10_QAGPT4_10P.json', 'r', encoding='utf-8') as file: \n",
    "#  with open('/srv/data1/fuxiaoqin/EnergyPlus/engineering-reference_iqa_testGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "# with open('/srv/data1/fuxiaoqin/Downloads/EngineeringReference1_6_TESTQAGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "# with open('/home/fuxiaoqin/Downloads/EngineeringReference1_All_TESTQAGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "with open('/srv/data1/fuxiaoqin/EnergyPlus/EI_iqa_testGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "    json_obj = json.load(file)  \n",
    "\n",
    "# print(json_obj)\n",
    "# print(json_obj[0])\n",
    "# data_chunkis = []\n",
    "data_instructions = []\n",
    "data_inputs = []\n",
    "data_outputs = []\n",
    "for piece in json_obj:\n",
    "    # print(\"piece\", piece['input'])\n",
    "    # data_chunkis.append(piece['chunki'])\n",
    "    data_instructions.append(piece['instruction'])\n",
    "    data_inputs.append(piece['input'])\n",
    "    data_outputs.append(piece['output'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfd9194-6f29-4be8-b6a7-890baedc9474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "261\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "print(len(data_instructions))\n",
    "print(len(data_inputs))\n",
    "print(len(data_outputs))\n",
    "# print(data_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86019347-ffb6-412a-b4c4-b75db72a163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip uninstall -q wandb\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b7b294-7331-41ce-af34-9a1ba2b8d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# from accelerate import PartialState\n",
    "# device_map={\"\": PartialState().process_index}\n",
    "# # device_index = Accelerator().process_index\n",
    "# # device_map = {\"\": device_index}\n",
    "# # Load model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     quantization_config=bnb_config, # BitsAndBytesConfig(load_in_8bit=True), #bnb_config,\n",
    "#     device_map=device_map,\n",
    "#     attn_implementation=attn_implementation,\n",
    "# )\n",
    "\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab7bf21a-5d10-485c-b588-76cc1c18650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_csv(fileName, instructions, inputs, outputs, predictions):\n",
    "    min_len = len(instructions) # 296 # 11 # 129 # 156 #2961 # 1297 #129 #296 # 359 # min (len(instructions), len(inputs), len(outputs), len(predictions))\n",
    "    wf = open(fileName, 'w', encoding=\"utf8\")\n",
    "    wf.write(\"Instruction,Input,Output,Predicted\\n\")\n",
    "    for i in range(min_len):\n",
    "        wf.write(str(instructions[i])+','+str(inputs[i])+',\"'+str(outputs[i])+'\",\"'+str(predictions[i])+'\"\\n')\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fcea778-ffbf-43ae-8f1a-d606e75e1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(references) 261\n"
     ]
    }
   ],
   "source": [
    "references = data_outputs\n",
    "print(\"len(references)\", len(references))\n",
    "# print(\"len(pretrained_responses)\", len(pretrained_responses))\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from numpy import *\n",
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    reference = [reference.split()]\n",
    "    hypothesis = hypothesis.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "def save_metrics_csv(fileName, trainTime, bleus, rouge1, rouge2, rouge3, b_r, b_p, b_f):\n",
    "    bleu = mean(bleus)\n",
    "    r1_r=rouge1['r']\n",
    "    r1_p=rouge1['p']\n",
    "    r1_f=rouge1['f']\n",
    "    r2_r=rouge2['r']\n",
    "    r2_p=rouge2['p']\n",
    "    r2_f=rouge2['f']\n",
    "    r3_r=rouge3['r']\n",
    "    r3_p=rouge3['p']\n",
    "    r3_f=rouge3['f']    \n",
    "    b_r = str(b_r).replace(\"tensor(\",\"\").replace(\")\",\"\")\n",
    "    b_p = str(b_p).replace(\"tensor(\",\"\").replace(\")\",\"\")\n",
    "    b_f = str(b_f).replace(\"tensor(\",\"\").replace(\")\",\"\")\n",
    "    if os.path.exists(fileName):\n",
    "        r = open(fileName, 'a', encoding=\"utf8\")\n",
    "        r.write(str(trainTime)+\",\"+str(bleu)+\",\"+str(r1_r)+\",\"+str(r1_p)+\",\"+str(r1_f)+\",\"+str(r2_r)+\",\"+str(r2_p)+\",\"+str(r2_f)+\",\"+str(r3_r)+\",\"+str(r3_p)+\",\"+str(r3_f)+\",\"+str(b_r)+\",\"+str(b_p)+\",\"+str(b_f)+\",\\n\")\n",
    "        r.close()\n",
    "    else:\n",
    "        r = open(fileName, 'w', encoding=\"utf8\")\n",
    "        r.write(\"Training,BLEU,ROUGE-1,,,ROUGE-2,,,ROUGE-L,,,BERT,,,Fineturning\\n\")\n",
    "        r.write(\"Time,,r ,p,f,r ,p,f,r ,p,f,R,P,F1,Time\\n\")\n",
    "        r.write(str(trainTime)+\",\"+str(bleu)+\",\"+str(r1_r)+\",\"+str(r1_p)+\",\"+str(r1_f)+\",\"+str(r2_r)+\",\"+str(r2_p)+\",\"+str(r2_f)+\",\"+str(r3_r)+\",\"+str(r3_p)+\",\"+str(r3_f)+\",\"+str(b_r)+\",\"+str(b_p)+\",\"+str(b_f)+\",\\n\")\n",
    "        r.close()  \n",
    "        \n",
    "def compute_save_metrics(references, responses, fileName, trainingTime):\n",
    "    # BLEU scores\n",
    "    bleu_scores = [compute_bleu(ref, hypo) for ref, hypo in zip(references, responses)]\n",
    "    print(\"BLEU scores with fineturning:\", mean(bleu_scores))\n",
    "    \n",
    "    # ROUGE scores\n",
    "    from rouge import Rouge\n",
    "    rouge = Rouge()\n",
    "    rouge1_r=0\n",
    "    rouge1_p=0\n",
    "    rouge1_f=0\n",
    "    rouge2_r=0\n",
    "    rouge2_p=0\n",
    "    rouge2_f=0\n",
    "    rougel_r=0\n",
    "    rougel_p=0\n",
    "    rougel_f=0\n",
    "    rouge1 = {'r':0, 'p':0, 'f':0}\n",
    "    rouge2 = {'r':0, 'p':0, 'f':0}\n",
    "    rougel = {'r':0, 'p':0, 'f':0}\n",
    "    try:\n",
    "        rouge_scoress = rouge.get_scores(responses, references)\n",
    "        for rouge_scores in rouge_scoress:\n",
    "            rouge1=rouge_scores['rouge-1']\n",
    "            rouge1_r += rouge1['r']\n",
    "            rouge1_p += rouge1['p']\n",
    "            rouge1_f += rouge1['f']\n",
    "            rouge2=rouge_scores['rouge-2']   \n",
    "            rouge2_r += rouge2['r']\n",
    "            rouge2_p += rouge2['p']\n",
    "            rouge2_f += rouge2['f']\n",
    "            rougel=rouge_scores['rouge-l']       \n",
    "            rougel_r += rougel['r']\n",
    "            rougel_p += rougel['p']\n",
    "            rougel_f += rougel['f']\n",
    "        lenRouge = len(rouge_scoress)\n",
    "        rouge1_r /= lenRouge\n",
    "        rouge1_p /= lenRouge\n",
    "        rouge1_f /= lenRouge\n",
    "        rouge2_r /= lenRouge\n",
    "        rouge2_p /= lenRouge\n",
    "        rouge2_f /= lenRouge\n",
    "        rougel_r /= lenRouge\n",
    "        rougel_p /= lenRouge\n",
    "        rougel_f /= lenRouge        \n",
    "    except:\n",
    "        print(\"No rouge scores\")\n",
    "    # ROUGE-1 Scores\n",
    "    print(f\"ROUGE-1 scores r, p, f with fine-turning: {rouge1_r}, {rouge1_p}, {rouge1_f} \\n\")    \n",
    "    # ROUGE-2 scores\n",
    "    print(f\"ROUGE-2 scores r, p, f withfine-turning: {rouge2_r}, {rouge2_p}, {rouge2_f} \\n\")    \n",
    "    # ROUGE-L scores\n",
    "    print(f\"ROUGE-l scores r, p, f with fine-turning: {rougel_r}, {rougel_p}, {rougel_f} \\n\")\n",
    "    # ROUGE-W scores    \n",
    "    \n",
    "    #  BERTScore\n",
    "    from bert_score import score\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(responses, references, lang=\"en\", rescale_with_baseline=True, verbose=True)\n",
    "    print(f\"type(P): {type(P)}\\n\")\n",
    "    P = torch.mean(P)\n",
    "    R = torch.mean(R)\n",
    "    F1 = torch.mean(F1)\n",
    "    # Print Rbert, Pbert and Fbert\n",
    "    \n",
    "    print(f\"BERTScore-Precision with fineturning: {P}\\n\")\n",
    "    print(f\"\\nBERTScore-Recall with fineturning: {R}\\n\")\n",
    "    print(f\"BERTScore-F-Measure with fineturning: {F1}\")\n",
    "\n",
    "    save_metrics_csv(fileName, trainingTime, bleu_scores, rouge1, rouge2, rougel, P, R, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da74f69-aec9-46ce-9ddd-1ab476cdfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(model, data_inputs):\n",
    "    responses = []\n",
    "    for dp in data_inputs:\n",
    "        inputs = tokenizer(dp, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        text = response_text.replace(dp,\"\").replace(\"#\",\" \").replace(\"\\n\",\" \")\n",
    "        # text = response_text.replace(dp,\"\").replace(\"#\",\" \").replace(\"\\n\",\" \").split(\", \")[0].split(\". \")[0].strip()+\".\"\n",
    "        # if len(text)<3:\n",
    "        #     text = response_text.replace(dp,\"\").replace(\"#\",\" \").replace(\"\\n\",\" \")\n",
    "        print(f\"text={text}\")\n",
    "        response = text\n",
    "        responses.append(response)\n",
    "    print(f\"responses = {responses}\")\n",
    "    print(\"len(responses) =\", len(responses))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c78a02-6d16-4821-a922-4e1643e9f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_responses = get_responses(model, data_inputs)\n",
    "# # # save_results_csv(\"/srv/data1/fuxiaoqin/Results_10_LM32_1B_10P.csv\", data_instructions, data_inputs, data_outputs, pretrained_responses)\n",
    "# # # compute_save_metrics(references, pretrained_responses, \"/srv/data1/fuxiaoqin/Metrics_10_LM32_1B_10P.csv\",\"LM32_1B\")\n",
    "\n",
    "# save_results_csv(\"/srv/data1/fuxiaoqin/Results_E_27_11B_before.csv\", data_instructions, data_inputs, data_outputs, pretrained_responses)\n",
    "# compute_save_metrics(references, pretrained_responses, \"/srv/data1/fuxiaoqin/Metrics_E_27_11B_before.csv\",\"LM32_11B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e854da3d-eade-4ca6-b02d-7e5b1d95bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd9f8a54-7564-4a4c-87f5-45c62c7c9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_loraplus_optimizer\n",
    "from transformers import Trainer\n",
    "import bitsandbytes as bnb\n",
    "# from peft.optimizers import create_lorafa_optimizer\n",
    "from transformers import Trainer, get_cosine_schedule_with_warmup\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8477385a-004e-47a1-b942-6f4b6cc38dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"+tokenizer.eos_token\n",
    "    return text\n",
    "        \n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"+tokenizer.eos_token\n",
    "    return text\n",
    "        \n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af3ec37-808f-4557-9afa-9319c978ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt2(prompt, max_length):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b25bb3ee-49e1-478d-ae24-5af8f1867062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, EvaConfig\n",
    "peft_config = LoraConfig(\n",
    "    # init_lora_weights = \"eva\",\n",
    "    # eva_config = EvaConfig(rho = 2.0),\n",
    "\n",
    "    # use_dora=True,\n",
    "    r = 1,  #16, #128,  #32, #16, # 1,  #16\n",
    "    lora_alpha = 1,  #32, #64, #32, # 1, #32,\n",
    "    lora_dropout=0.05,\n",
    "\n",
    "    # use_rslora = True,\n",
    "\n",
    "        \n",
    "    bias= \"none\", # \"all\", # \"none\",\n",
    "    # task_type=\"QUESTION_ANS\",     # for 1B and 3B\n",
    "    task_type=\"CAUSAL_LM\",   # for 11B\n",
    "    # target_modules=['k_proj', 'q_proj', 'v_proj'], # for 11B\n",
    "    # target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj', 'fc1', 'fc2'], # for 11B\n",
    "    target_modules=['q_proj'], #  'fc1', 'fc2'], # for 11B\n",
    "    # target_modules=\"all-linear\", # for 1B and 3B\n",
    "    # modules_to_save=[\"embed_tokens\", \"lm_head\"]\n",
    "        # target_modules=['q_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8c979-6ade-4dba-85a1-2419c9dbcd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_loraplus_optimizer\n",
    "\n",
    "def QA_training(model, QA_folder, QA_file, epochs=1):\n",
    "    data_dict = {\n",
    "        # \"train\": os.path.join(\"/home/fuxiaoqin/Downloads/\", \"A311.json\")\n",
    "        # \"train\": os.path.join(\"/mnt/data1/fuxiaoqin/Downloads/\", \"Ashrae_2.json\")\n",
    "        \"train\": os.path.join(QA_folder, QA_file)\n",
    "        # \"train\": os.path.join(\"/home/fuxiaoqin/Downloads/\", \"EngineeringReference_QAGPT4.json\")\n",
    "    }\n",
    "  \n",
    "                \n",
    "    dataset = load_dataset(\"json\", data_files=data_dict, split = \"train\")\n",
    "\n",
    "    dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "    \n",
    "\n",
    "    #     return tokenizer(formatting_func(prompt))\n",
    "    dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "    \n",
    "\n",
    "    \n",
    "    max_length =512 \n",
    "    \n",
    "    \n",
    "    dataset = dataset.map(generate_and_tokenize_prompt2, max_length)\n",
    "    # dataset['text'][3]\n",
    "    \n",
    "    # dataset = dataset.train_test_split(test_size=0.1)\n",
    "    print(dataset)\n",
    "    \n",
    "\n",
    "    #     # target_modules=['q_proj']\n",
    "    # )\n",
    "    model = get_peft_model(model, peft_config)   \n",
    "    # model = torch.nn.DataParallel(model, device_ids=[0,1,2])\n",
    "\n",
    "    optimizer = create_loraplus_optimizer(\n",
    "        model=model,\n",
    "        optimizer_cls=bnb.optim.Adam8bit,\n",
    "        lr=5e-5,\n",
    "        loraplus_lr_ratio=16,\n",
    "    )\n",
    "    scheduler = None\n",
    "\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=new_model,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        # optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        # eval_strategy=\"steps\",\n",
    "        # eval_steps=0.2,\n",
    "        logging_steps=10,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=1e-5,\n",
    "        fp16=False, # True, #False,\n",
    "        bf16=True, # False, #True,\n",
    "        group_by_length=True,\n",
    "        # report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        # eval_dataset=dataset[\"test\"],\n",
    "        peft_config=peft_config,\n",
    "        # max_seq_length=512,\n",
    "        # dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        # use_dora=True,\n",
    "        # packing= False,\n",
    "    )\n",
    "\n",
    "    print(torch.cuda.is_available())\n",
    "    for i in range(epochs):\n",
    "        trainer.train()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d213c1a-f1f8-4780-b84f-9d30170db490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model = QA_training(model, \"/srv/data1/fuxiaoqin/inputs/\", \"engineering-reference_iqa.json\",1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "lora_time = end_time - start_time\n",
    "print(\"Fine-tuning: {:.2f}seconds\".format(lora_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d987a8-b57b-4f99-9a4e-bd05f0a1dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Fine-tuning: {:.2f}seconds\".format(lora_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424810f5-b523-4238-b045-c9f28e5c70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = get_responses(model, data_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c909a14-07ea-47e6-933a-b08fe30354f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58df9b6-810c-439c-8517-dfabb38a9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_results_csv(\"/srv/data1/fuxiaoqin/Results_10_LM32_1B_10P_QA.csv\", data_instructions, data_inputs, data_outputs, responses)\n",
    "# compute_save_metrics(references, responses, \"/srv/data1/fuxiaoqin/Metrics_10_LM32_1B_10P_QA.csv\",\"LM32_1B\")\n",
    "save_results_csv(\"/srv/data1/fuxiaoqin/Results_E_27_11B.csv\", data_instructions, data_inputs, data_outputs, responses)\n",
    "compute_save_metrics(references, responses, \"/srv/data1/fuxiaoqin/Metrics_E_27_11B.csv\",str(lora_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
