{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e58a3-c400-4b83-a865-17249edec364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install peft wandb\n",
    "# !{sys.executable} -m pip install pandas>=2.0.0\n",
    "import os\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # for 1B\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5742e-3e82-491a-8fa5-90b613457b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import MllamaForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig,LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datetime import datetime\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62edad2-5e84-419b-a181-dc9deca7a7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac3fdfd-43c1-4f48-854d-21bb9fb726ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# base_model = \"meta-llama/Llama-3.2-90B-Vision-Instruct\"\n",
    "# base_model = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "base_model = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "# base_model = \"meta-llama/Llama-3.2-1B\"\n",
    "# base_model = \"meta-llama/Llama-3.2-3B\"\n",
    "# base_model = \"meta-llama/Meta-Llama-3.1-70B\"\n",
    "# base_model = \"meta-llama/Meta-Llama-3.1-405B\"\n",
    "new_model = \"llama-32-11B_chat-building\"\n",
    "import torch\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "# !{sys.executable} -m pip install bitsandbytes\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdd413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "261\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#  with open('/srv/data1/fuxiaoqin/Downloads/EngineeringReference1_10_QAGPT4_10P.json', 'r', encoding='utf-8') as file: \n",
    "#  with open('/srv/data1/fuxiaoqin/EnergyPlus/engineering-reference_iqa_testGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "# with open('/srv/data1/fuxiaoqin/Downloads/EngineeringReference1_6_TESTQAGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "# with open('/home/fuxiaoqin/Downloads/EngineeringReference1_All_TESTQAGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "with open('/srv/data1/fuxiaoqin/EnergyPlus/EI_iqa_testGPT4omini.json', 'r', encoding='utf-8') as file:  \n",
    "    json_obj = json.load(file)  \n",
    "\n",
    "# print(json_obj)\n",
    "# print(json_obj[0])\n",
    "# data_chunkis = []\n",
    "data_instructions = []\n",
    "data_inputs = []\n",
    "data_outputs = []\n",
    "for piece in json_obj:\n",
    "    # print(\"piece\", piece['input'])\n",
    "    # data_chunkis.append(piece['chunki'])\n",
    "    data_instructions.append(piece['instruction'])\n",
    "    data_inputs.append(piece['input'])\n",
    "    data_outputs.append(piece['output'])\n",
    "\n",
    "print(len(data_instructions))\n",
    "print(len(data_inputs))\n",
    "print(len(data_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7175f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "# model = LlamaForCausalLM.from_pretrained(base_model, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddeb15cf-f0cb-4796-9959-fa2f7b487210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2e8aa14f2d4a7c8c2e7b7d19a2584e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "# 11B \n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config, # BitsAndBytesConfig(load_in_8bit=True), #bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b9fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_csv(fileName, instructions, inputs, outputs, predictions):\n",
    "    min_len = len(instructions)  # 296 # 46 # 128 # 156 #2961 # 1297 #129 #296 # 359 #\n",
    "    # min_len = min(len(instructions), len(inputs), len(predictions))\n",
    "    wf = open(fileName, 'w', encoding=\"utf8\")\n",
    "    wf.write(\"Instruction,Input,Output,Predicted\\n\")\n",
    "    for i in range(min_len):\n",
    "        wf.write(str(instructions[i])+','+str(inputs[i])+',\"'+str(outputs[i])+'\",\"'+str(predictions[i])+'\"\\n')\n",
    "    wf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b841d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(references) 261\n"
     ]
    }
   ],
   "source": [
    "references = data_outputs\n",
    "print(\"len(references)\", len(references))\n",
    "# print(\"len(pretrained_responses)\", len(pretrained_responses))\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from numpy import *\n",
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    reference = [reference.split()]\n",
    "    hypothesis = hypothesis.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "def save_metrics_csv(fileName, trainTime, bleus, rouge1, rouge2, rouge3, b_r, b_p, b_f):\n",
    "    bleu = mean(bleus)\n",
    "    r1_r=rouge1['r']\n",
    "    r1_p=rouge1['p']\n",
    "    r1_f=rouge1['f']\n",
    "    r2_r=rouge2['r']\n",
    "    r2_p=rouge2['p']\n",
    "    r2_f=rouge2['f']\n",
    "    r3_r=rouge3['r']\n",
    "    r3_p=rouge3['p']\n",
    "    r3_f=rouge3['f']    \n",
    "    b_r = str(b_r).replace(\"tensor(\",\"\").replace(\")\",\"\")\n",
    "    b_p = str(b_p).replace(\"tensor(\",\"\").replace(\")\",\"\")\n",
    "    b_f = str(b_f).replace(\"tensor(\",\"\").replace(\")\",\"\")\n",
    "    if os.path.exists(fileName):\n",
    "        r = open(fileName, 'a', encoding=\"utf8\")\n",
    "        r.write(str(trainTime)+\",\"+str(bleu)+\",\"+str(r1_r)+\",\"+str(r1_p)+\",\"+str(r1_f)+\",\"+str(r2_r)+\",\"+str(r2_p)+\",\"+str(r2_f)+\",\"+str(r3_r)+\",\"+str(r3_p)+\",\"+str(r3_f)+\",\"+str(b_r)+\",\"+str(b_p)+\",\"+str(b_f)+\",\\n\")\n",
    "        r.close()\n",
    "    else:\n",
    "        r = open(fileName, 'w', encoding=\"utf8\")\n",
    "        r.write(\"Training,BLEU,ROUGE-1,,,ROUGE-2,,,ROUGE-L,,,BERT,,,Fineturning\\n\")\n",
    "        r.write(\"Time,,r ,p,f,r ,p,f,r ,p,f,R,P,F1,Time\\n\")\n",
    "        r.write(str(trainTime)+\",\"+str(bleu)+\",\"+str(r1_r)+\",\"+str(r1_p)+\",\"+str(r1_f)+\",\"+str(r2_r)+\",\"+str(r2_p)+\",\"+str(r2_f)+\",\"+str(r3_r)+\",\"+str(r3_p)+\",\"+str(r3_f)+\",\"+str(b_r)+\",\"+str(b_p)+\",\"+str(b_f)+\",\\n\")\n",
    "        r.close()  \n",
    "        \n",
    "def compute_save_metrics(references, responses, fileName, trainingTime):\n",
    "    # BLEU scores\n",
    "    bleu_scores = [compute_bleu(ref, hypo) for ref, hypo in zip(references, responses)]\n",
    "    print(\"BLEU scores with fineturning:\", mean(bleu_scores))\n",
    "    \n",
    "    # ROUGE scores\n",
    "    from rouge import Rouge\n",
    "    rouge = Rouge()\n",
    "    rouge1_r=0\n",
    "    rouge1_p=0\n",
    "    rouge1_f=0\n",
    "    rouge2_r=0\n",
    "    rouge2_p=0\n",
    "    rouge2_f=0\n",
    "    rougel_r=0\n",
    "    rougel_p=0\n",
    "    rougel_f=0\n",
    "    rouge1 = {'r':0, 'p':0, 'f':0}\n",
    "    rouge2 = {'r':0, 'p':0, 'f':0}\n",
    "    rougel = {'r':0, 'p':0, 'f':0}\n",
    "    try:\n",
    "        rouge_scoress = rouge.get_scores(responses, references)\n",
    "        for rouge_scores in rouge_scoress:\n",
    "            rouge1=rouge_scores['rouge-1']\n",
    "            rouge1_r += rouge1['r']\n",
    "            rouge1_p += rouge1['p']\n",
    "            rouge1_f += rouge1['f']\n",
    "            rouge2=rouge_scores['rouge-2']   \n",
    "            rouge2_r += rouge2['r']\n",
    "            rouge2_p += rouge2['p']\n",
    "            rouge2_f += rouge2['f']\n",
    "            rougel=rouge_scores['rouge-l']       \n",
    "            rougel_r += rougel['r']\n",
    "            rougel_p += rougel['p']\n",
    "            rougel_f += rougel['f']\n",
    "        lenRouge = len(rouge_scoress)\n",
    "        rouge1_r /= lenRouge\n",
    "        rouge1_p /= lenRouge\n",
    "        rouge1_f /= lenRouge\n",
    "        rouge2_r /= lenRouge\n",
    "        rouge2_p /= lenRouge\n",
    "        rouge2_f /= lenRouge\n",
    "        rougel_r /= lenRouge\n",
    "        rougel_p /= lenRouge\n",
    "        rougel_f /= lenRouge        \n",
    "    except:\n",
    "        print(\"No rouge scores\")\n",
    "    # ROUGE-1 Scores\n",
    "    print(f\"ROUGE-1 scores r, p, f with fine-turning: {rouge1_r}, {rouge1_p}, {rouge1_f} \\n\")    \n",
    "    # ROUGE-2 scores\n",
    "    print(f\"ROUGE-2 scores r, p, f withfine-turning: {rouge2_r}, {rouge2_p}, {rouge2_f} \\n\")    \n",
    "    # ROUGE-L scores\n",
    "    print(f\"ROUGE-l scores r, p, f with fine-turning: {rougel_r}, {rougel_p}, {rougel_f} \\n\")\n",
    "    # ROUGE-W scores    \n",
    "    \n",
    "    #  BERTScore\n",
    "    from bert_score import score\n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = score(responses, references, lang=\"en\", rescale_with_baseline=True, verbose=True)\n",
    "    print(f\"type(P): {type(P)}\\n\")\n",
    "    P = torch.mean(P)\n",
    "    R = torch.mean(R)\n",
    "    F1 = torch.mean(F1)\n",
    "    # Print Rbert, Pbert and Fbert\n",
    "    \n",
    "    print(f\"BERTScore-Precision with fineturning: {P}\\n\")\n",
    "    print(f\"\\nBERTScore-Recall with fineturning: {R}\\n\")\n",
    "    print(f\"BERTScore-F-Measure with fineturning: {F1}\")\n",
    "\n",
    "    save_metrics_csv(fileName, trainingTime, bleu_scores, rouge1, rouge2, rougel, P, R, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab37bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(model, data_inputs):\n",
    "    responses = []\n",
    "    for dp in data_inputs:\n",
    "        inputs = tokenizer(dp, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        text = response_text.replace(dp,\"\").replace(\"#\",\" \").replace(\"\\n\",\" \").split(\", \")[0].split(\". \")[0].strip()+\".\"\n",
    "        if len(text)<3:\n",
    "            text = response_text.replace(dp,\"\").replace(\"#\",\" \").replace(\"\\n\",\" \")\n",
    "\n",
    "        \n",
    "        # text = response_text.replace(dp,\"\").replace(\"#\",\" \").replace(\"\\n\",\" \")\n",
    "        \n",
    "        print(f\"text={text}\")\n",
    "        response = text\n",
    "        responses.append(response)\n",
    "    print(f\"responses = {responses}\")\n",
    "    print(\"len(responses) =\", len(responses))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c90ac306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_loraplus_optimizer\n",
    "from transformers import Trainer\n",
    "import bitsandbytes as bnb\n",
    "# from peft.optimizers import create_lorafa_optimizer\n",
    "from transformers import Trainer, get_cosine_schedule_with_warmup\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d541f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "def CIQAFrom4Lines(line, nextLine, thirdLine, fouthLine):\n",
    "    oneCIQA = {\"context\": \"\", \"instruction\":\"\", \"question\":\"\", \"answer\":\"\" }\n",
    "    if not \"context\" in str(line).lower():\n",
    "        return oneCIQA, oneTriple\n",
    "    line2 = str(line).replace('\"context\": ', '').replace('\"instruction\": ', '').replace('\"question\": ', '').replace('\"answer\": ', '').replace('\"Q\": ', '').replace('\"A\": ', '').replace('\"q\": ', '').replace('\"a\": ', '').replace('{', '').replace('}', '').replace('\"', '').replace(',', '').replace('\\n', '')\n",
    "    nextLine2 = str(nextLine).replace('\"context\": ', '').replace('\"instruction\": ', '').replace('\"question\": ', '').replace('\"answer\": ', '').replace('\"Q\": ', '').replace('\"A\": ', '').replace('\"q\": ', '').replace('\"a\": ', '').replace('{', '').replace('}', '').replace('\"', '').replace(',', '').replace('\\n', '')\n",
    "    thirdLine2 = str(thirdLine).replace('\"context\": ', '').replace('\"instruction\": ', '').replace('\"question\": ', '').replace('\"answer\": ', '').replace('\"Q\": ', '').replace('\"A\": ', '').replace('\"q\": ', '').replace('\"a\": ', '').replace('{', '').replace('}', '').replace('\"', '').replace(',', '').replace('\\n', '')\n",
    "    fouthLine2 = str(fouthLine).replace('\"context\": ', '').replace('\"instruction\": ', '').replace('\"question\": ', '').replace('\"answer\": ', '').replace('\"Q\": ', '').replace('\"A\": ', '').replace('\"q\": ', '').replace('\"a\": ', '').replace('{', '').replace('}', '').replace('\"', '').replace(',', '').replace('\\n', '')\n",
    "      \n",
    "    if len(line2)>3 and len(nextLine2)>3 and len(thirdLine2)>0 and len(fouthLine2)>0:  \n",
    "        oneCIQA  = {\"context\": line2.strip(), \"instruction\": nextLine2.strip(), \"question\": thirdLine2.strip(), \"answer\":fouthLine2.strip() } \n",
    "    return oneCIQA\n",
    "\n",
    "def readTextFile(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    return lines\n",
    "\n",
    "def read_raw_data(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    raw_data = []\n",
    "    lenlines = len(lines)\n",
    "    i = 0\n",
    "    while i < lenlines:\n",
    "        line = lines[i]\n",
    "        line_lower = line.lower().strip()\n",
    "        if '\"context\": '  in line_lower:\n",
    "            i  += 1\n",
    "            if i >= lenlines:\n",
    "                break\n",
    "            nextLine = lines[i]\n",
    "            i  += 1\n",
    "            if i >= lenlines:\n",
    "                break\n",
    "            thirdLine = lines[i]\n",
    "            i  += 1\n",
    "            if i >= lenlines:\n",
    "                break\n",
    "            fouthLine = lines[i]\n",
    "            oneCIQA = CIQAFrom4Lines(line, nextLine, thirdLine, fouthLine)\n",
    "            if len(oneCIQA['context'])>0 and len(oneCIQA['instruction'])>0 and len(oneCIQA['question'])>0 and len(oneCIQA['answer'])>0:\n",
    "                raw_data.append(oneCIQA)\n",
    "        i += 1\n",
    "    return raw_data\n",
    "\n",
    "def saveTextToFile(data, outputFile):\n",
    "    # print(data)\n",
    "    with open(outputFile, \"w\") as outfile:\n",
    "        outfile.write(data)\n",
    "    outfile.close()\n",
    "\n",
    "def appendTextToFile(data, outputFile):\n",
    "    # print(data)\n",
    "    with open(outputFile, \"a\") as outfile:\n",
    "        outfile.write(data)\n",
    "    outfile.close()\n",
    "\n",
    "# lines = readTextFile(\"/UAStudy/EnergyPlus/doc/overview_ciqa_nochunk.txt\")\n",
    "# raw_data = str(lines)\n",
    "# raw_data = read_raw_data(\"/UAStudy/EnergyPlus/doc/overview_ciqa_nochunk.txt\")\n",
    "# print(\"raw_data\", raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3bb2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Prompt formatting\n",
    "def format_prompt(example):\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"Context: {example['context']}\\n\"\n",
    "        f\"Instruction: {example['instruction']}\\n\"\n",
    "        f\"Question: {example['question']}<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        f\"{example['answer']}<|eot_id|>\"\n",
    "    )\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    prompt = format_prompt(example)\n",
    "    encoded = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=0.2,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    fp16=False, # True, #False,\n",
    "    bf16=True, # False, #True,\n",
    "    group_by_length=True,\n",
    "    # report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41b74da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, EvaConfig\n",
    "peft_config = LoraConfig(\n",
    "    # init_lora_weights = \"eva\",\n",
    "    # eva_config = EvaConfig(rho = 2.0),\n",
    "\n",
    "    use_dora=True,\n",
    "    r = 128,  #16, #128,  #32, #16, # 1,  #16\n",
    "    lora_alpha = 64, #32, #64, #32, # 1, #32,\n",
    "    lora_dropout=0.05,\n",
    "\n",
    "    # use_rslora = True,\n",
    "\n",
    "        \n",
    "    bias=\"all\", # \"none\",\n",
    "    # task_type=\"QUESTION_ANS\",     # for 1B and 3B\n",
    "    task_type=\"CAUSAL_LM\",   # for 11B\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj', 'fc1', 'fc2'], # for 11B\n",
    "    # target_modules=['q_proj'], #  'fc1', 'fc2'], # for 11B\n",
    "    # target_modules=\"all-linear\", # for 1B and 3B\n",
    "    # modules_to_save=[\"embed_tokens\", \"lm_head\"]\n",
    "        # target_modules=['q_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_loraplus_optimizer\n",
    "\n",
    "def QA_training(model, QA_file, epochs=1):\n",
    "    raw_data = read_raw_data(QA_file)\n",
    "    dataset = Dataset.from_list(raw_data)            \n",
    "    tokenized_dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    \n",
    "    max_length =512 \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #     # target_modules=['q_proj']\n",
    "    # )\n",
    "    model = get_peft_model(model, peft_config)   \n",
    "    # model = torch.nn.DataParallel(model, device_ids=[0,1,2])\n",
    "\n",
    "    optimizer = create_loraplus_optimizer(\n",
    "        model=model,\n",
    "        optimizer_cls=bnb.optim.Adam8bit,\n",
    "        lr=5e-5,\n",
    "        loraplus_lr_ratio=16,\n",
    "    )\n",
    "    scheduler = None\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        trainer.train()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6850b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model = QA_training(model, \"/srv/data1/fuxiaoqin/EnergyPlus/surface-heat-balance-manager-processes_ciqa_nochunk.txt\",2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "lora_time = end_time - start_time\n",
    "print(\"Fine-tuning: {:.2f}seconds\".format(lora_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38c555f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning: 3641.73seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning: {:.2f}seconds\".format(lora_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = get_responses(model, data_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15e3a005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_csv(\"/srv/data1/fuxiaoqin/Results_C3_24_11B.csv\", data_instructions, data_inputs, data_outputs, responses)\n",
    "compute_save_metrics(references, responses, \"/srv/data1/fuxiaoqin/Metrics_C3_24_11B.csv\",str(lora_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4114fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e4497-ab2a-49f3-adf8-8b7c3c2414da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a1913-2d7f-4c53-bf9a-977bc4bf26c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78d363-f976-4bcb-967d-2ec746639643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
